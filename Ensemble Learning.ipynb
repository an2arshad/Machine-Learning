{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUgzddEMx0v+NkKB0Mqfvb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Ensemble Learning**\n","\n","**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n","- **Answer:**\n","  - Ensemble Learning is a machine learning paradigm that combines multiple models to improve overall performance.\n","  - The key idea is that by aggregating the predictions of several models, the ensemble can achieve better accuracy and robustness than any individual model.\n","\n","**Question 2: What is the difference between Bagging and Boosting?**\n","- **Answer:**\n","  - **Bagging (Bootstrap Aggregating):**\n","    - Involves training multiple models independently on different subsets of the data.\n","    - Reduces variance and helps to avoid overfitting.\n","  - **Boosting:**\n","    - Models are trained sequentially, with each new model focusing on the errors made by the previous ones.\n","    - Aims to reduce bias and improve accuracy.\n","\n","**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n","- **Answer:**\n","  - Bootstrap sampling is a technique where multiple subsets of the training data are created by randomly sampling with replacement.\n","  - In Bagging methods like Random Forest, bootstrap samples are used to train individual trees, allowing for diversity among the models and improving the ensemble's performance.\n","\n","**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n","- **Answer:**\n","  - OOB samples are the data points that are not included in a bootstrap sample for a particular model.\n","  - The OOB score is calculated by using these samples to evaluate the model's performance, providing an unbiased estimate of the model's accuracy without needing a separate validation set.\n","\n","**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n","- **Answer:**\n","  - **Single Decision Tree:**\n","    - Feature importance is determined based on the reduction in impurity (e.g., Gini impurity) when a feature is used for splitting.\n","  - **Random Forest:**\n","    - Feature importance is averaged over all trees in the forest, providing a more stable and reliable measure of feature significance.\n","\n"],"metadata":{"id":"YBq1RR-G9blK"}},{"cell_type":"code","source":["#Question 6: Write a Python program to load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer(), train a Random Forest Classifier, and print the top 5 most important features based on feature importance scores.\n","\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load dataset\n","data = load_breast_cancer()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Train Random Forest Classifier\n","model = RandomForestClassifier()\n","model.fit(X, y)\n","\n","# Get feature importance\n","importance = model.feature_importances_\n","indices = importance.argsort()[::-1]\n","\n","# Print top 5 features\n","print(\"Top 5 most important features:\")\n","for i in range(5):\n","    print(f\"{X.columns[indices[i]]}: {importance[indices[i]]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wslbjumf-DKs","executionInfo":{"status":"ok","timestamp":1756305678212,"user_tz":-330,"elapsed":3111,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"78986243-8094-44ca-f7b8-9fd3f79d5ec7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 most important features:\n","worst concave points: 0.14757269490949376\n","worst perimeter: 0.12289008517538373\n","worst radius: 0.1226654207911246\n","worst area: 0.09463671691228076\n","mean concave points: 0.08361913519901241\n"]}]},{"cell_type":"code","source":["#Question 7: Write a Python program to train a Bagging Classifier using Decision Trees on the Iris dataset and evaluate its accuracy compared with a single Decision Tree.\n","\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train single Decision Tree\n","tree = DecisionTreeClassifier()\n","tree.fit(X_train, y_train)\n","tree_pred = tree.predict(X_test)\n","tree_accuracy = accuracy_score(y_test, tree_pred)\n","\n","# Train Bagging Classifier\n","bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10)\n","bagging.fit(X_train, y_train)\n","bagging_pred = bagging.predict(X_test)\n","bagging_accuracy = accuracy_score(y_test, bagging_pred)\n","\n","print(f\"Single Decision Tree Accuracy: {tree_accuracy}\")\n","print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_xEjFcr-Zfv","executionInfo":{"status":"ok","timestamp":1756306044812,"user_tz":-330,"elapsed":32,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"312f629d-fed4-4d46-f45d-bf9f7819780c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Single Decision Tree Accuracy: 1.0\n","Bagging Classifier Accuracy: 1.0\n"]}]},{"cell_type":"code","source":["#Question 8: Write a Python program to train a Random Forest Classifier, tune hyperparameters max_depth and n_estimators using GridSearchCV, and print the best parameters and final accuracy.\n","\n","from sklearn.datasets import load_iris\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Set up parameter grid\n","param_grid = {\n","    'max_depth': [None, 10, 20, 30],\n","    'n_estimators': [10, 50, 100]\n","}\n","\n","# Initialize Random Forest Classifier\n","rf = RandomForestClassifier()\n","\n","# Set up GridSearchCV\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters and accuracy\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","final_accuracy = accuracy_score(y_test, best_model.predict(X_test))\n","\n","print(f\"Best Parameters: {best_params}\")\n","print(f\"Final Accuracy: {final_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygM1Iv7f_hbT","executionInfo":{"status":"ok","timestamp":1756306175752,"user_tz":-330,"elapsed":7413,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"7941e7c0-feee-4e8d-c13f-5bda5e2343d2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': None, 'n_estimators': 50}\n","Final Accuracy: 1.0\n"]}]},{"cell_type":"code","source":["#Question 9: Write a Python program to train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset and compare their Mean Squared Errors (MSE).\n","\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load dataset\n","housing = fetch_california_housing()\n","X = housing.data\n","y = housing.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Bagging Regressor\n","bagging_regressor = BaggingRegressor()\n","bagging_regressor.fit(X_train, y_train)\n","bagging_pred = bagging_regressor.predict(X_test)\n","bagging_mse = mean_squared_error(y_test, bagging_pred)\n","\n","# Train Random Forest Regressor\n","rf_regressor = RandomForestRegressor()\n","rf_regressor.fit(X_train, y_train)\n","rf_pred = rf_regressor.predict(X_test)\n","rf_mse = mean_squared_error(y_test, rf_pred)\n","\n","print(f\"Bagging Regressor MSE: {bagging_mse}\")\n","print(f\"Random Forest Regressor MSE: {rf_mse}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mn6fbA4MAVSI","executionInfo":{"status":"ok","timestamp":1756306236208,"user_tz":-330,"elapsed":26583,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"56c72d77-fc99-4ad5-8a36-9a2b42d831f8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor MSE: 0.2838521579979651\n","Random Forest Regressor MSE: 0.2522856202693707\n"]}]},{"cell_type":"markdown","source":["**Question 10: Explain your step-by-step approach to using ensemble techniques to predict loan default.**\n","- **Answer:**\n","  1. **Choose between Bagging or Boosting:**\n","     - Analyze the dataset for overfitting tendencies; if overfitting is a concern, consider Bagging (e.g., Random Forest).\n","     - If the focus is on improving accuracy and reducing bias, consider Boosting (e.g., AdaBoost).\n","  \n","  2. **Handle Overfitting:**\n","     - Use techniques like cross-validation, regularization, and pruning (for decision trees) to mitigate overfitting.\n","  \n","  3. **Select Base Models:**\n","     - Choose a diverse set of models (e.g., Decision Trees, Logistic Regression) to form the ensemble.\n","  \n","  4. **Evaluate Performance Using Cross-Validation:**\n","     - Implement k-fold cross-validation to assess the model's performance and ensure it generalizes well to unseen data.\n","  \n","  5. **Justify How Ensemble Learning Improves Decision-Making:**\n","     - Ensemble learning combines the strengths of multiple models, leading to more accurate predictions and better risk assessment in loan default predictions, ultimately aiding in informed decision-making."],"metadata":{"id":"ofaixwAnA6y3"}}]}