{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfkVFxBV0tXwcwib/9xOX6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **KNN & PCA**\n","---\n","\n","## Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n","\n","**Answer:**\n","\n","- **Definition:**  \n","  K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression. It is a non-parametric, instance-based learning method, meaning it does not assume any underlying data distribution and makes predictions based on the entire training dataset.\n","\n","- **Working Principle:**  \n","  The core idea is that similar data points tend to be close to each other in the feature space. For a new input sample, KNN finds the \\( K \\) closest training samples (neighbors) based on a distance metric (commonly Euclidean distance).\n","\n","- **In Classification:**  \n","  - The algorithm identifies the \\( K \\) nearest neighbors of the new data point.  \n","  - It then assigns the class label that is most frequent among these neighbors (majority voting).  \n","  - For example, if among 5 neighbors, 3 belong to class A and 2 to class B, the new point is classified as class A.\n","\n","- **In Regression:**  \n","  - Instead of voting, the algorithm takes the average (or weighted average) of the target values of the \\( K \\) nearest neighbors.  \n","  - This average is used as the predicted continuous value for the new data point.\n","\n","- **Advantages:**  \n","  - Simple to understand and implement.  \n","  - No training phase, making it fast to set up.  \n","  - Naturally handles multi-class problems.\n","\n","- **Disadvantages:**  \n","  - Computationally expensive during prediction for large datasets.  \n","  - Sensitive to irrelevant or redundant features and feature scaling.  \n","  - Performance degrades with high-dimensional data due to the curse of dimensionality.\n","\n","---\n","\n","## Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n","\n","**Answer:**\n","\n","- **Definition:**  \n","  The Curse of Dimensionality refers to the various challenges that arise when working with data in high-dimensional spaces. As the number of features (dimensions) increases, the volume of the space increases exponentially, causing data points to become sparse.\n","\n","- **Implications for KNN:**  \n","  - **Distance Concentration:** In high dimensions, the difference between the nearest and farthest neighbor distances becomes negligible. This makes it difficult for KNN to distinguish between close and distant points, reducing its effectiveness.  \n","  - **Sparsity:** With many dimensions, data points are spread thinly, so neighbors may not be truly \"close,\" leading to noisy or unreliable predictions.  \n","  - **Increased Computation:** More dimensions mean more calculations for distance, increasing computational cost.\n","\n","- **Effect on Model Performance:**  \n","  - KNN’s reliance on distance metrics means that as dimensionality grows, the notion of \"closeness\" loses meaning, causing poor classification or regression accuracy.  \n","  - Overfitting risk increases because the model may fit noise rather than meaningful patterns.\n","\n","- **Mitigation Strategies:**  \n","  - Dimensionality reduction techniques like PCA to reduce feature space.  \n","  - Feature selection to remove irrelevant features.  \n","  - Using distance metrics or algorithms designed for high-dimensional data.\n","\n","---\n","\n","## Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n","\n","**Answer:**\n","\n","- **Principal Component Analysis (PCA):**  \n","  PCA is a statistical technique used for dimensionality reduction by transforming the original correlated features into a smaller set of uncorrelated variables called principal components. These components capture the maximum variance in the data.\n","\n","- **How PCA Works:**  \n","  - Computes the covariance matrix of the data to understand feature relationships.  \n","  - Calculates eigenvalues and eigenvectors of the covariance matrix.  \n","  - Eigenvectors define directions (principal components) in the feature space, and eigenvalues quantify the variance along these directions.  \n","  - Projects the original data onto the top principal components, reducing dimensionality while preserving most of the variance.\n","\n","- **Difference from Feature Selection:**  \n","  - **PCA (Feature Extraction):** Creates new features by combining original features linearly. The new features (principal components) are orthogonal and capture variance. Original features are transformed and not directly interpretable.  \n","  - **Feature Selection:** Selects a subset of the original features based on criteria like correlation, importance, or statistical tests. The original features remain unchanged and interpretable.\n","\n","- **Summary:**  \n","  PCA reduces dimensionality by creating new features, while feature selection reduces dimensionality by choosing existing features.\n","\n","---\n","\n","## Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n","\n","**Answer:**\n","\n","- **Eigenvectors:**  \n","  In PCA, eigenvectors represent the directions in the feature space along which the data varies the most. Each eigenvector corresponds to a principal component, which is a new axis in the transformed space.\n","\n","- **Eigenvalues:**  \n","  Each eigenvector has an associated eigenvalue that measures the amount of variance in the data along that eigenvector’s direction. Larger eigenvalues indicate that the corresponding eigenvector captures more variance.\n","\n","- **Importance in PCA:**  \n","  - Eigenvectors define the new coordinate system for the data after transformation.  \n","  - Eigenvalues help rank the principal components by importance. Components with higher eigenvalues are retained because they explain more variance.  \n","  - By selecting components with the largest eigenvalues, PCA reduces dimensionality while preserving the most significant information.\n","\n","---\n","\n","## Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n","\n","**Answer:**\n","\n","- **Complementarity:**  \n","  - PCA reduces the dimensionality of the dataset by extracting the most informative features, which helps alleviate the curse of dimensionality.  \n","  - KNN relies on distance calculations, which become unreliable in high-dimensional spaces. PCA transforms the data into a lower-dimensional space where distances are more meaningful.  \n","  - PCA also removes noise and redundant features, improving KNN’s accuracy and efficiency.\n","\n","- **Pipeline Workflow:**  \n","  1. **Data Preprocessing:** Scale features to have zero mean and unit variance.  \n","  2. **Dimensionality Reduction:** Apply PCA to reduce the number of features while retaining most variance.  \n","  3. **Model Training:** Train KNN on the PCA-transformed data.  \n","  4. **Prediction:** Use the trained KNN model to classify or regress new data points in the reduced space.\n","\n","- **Benefits:**  \n","  - Improved computational efficiency due to fewer features.  \n","  - Enhanced model generalization and reduced overfitting.  \n","  - Better interpretability of model performance.\n","\n","---\n","\n"],"metadata":{"id":"LuDawBhdPTjk"}},{"cell_type":"code","source":["#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n","\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","# Load dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","# KNN without scaling\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred)\n","# KNN with scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","knn.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn.predict(X_test_scaled)\n","acc_scaling = accuracy_score(y_test, y_pred_scaled)\n","print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n","print(f\"Accuracy with scaling: {acc_scaling:.4f}\")\n","\n","'''\n","Explanation:\n","Feature scaling significantly improves KNN performance because KNN relies on distance metrics sensitive to feature scales.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbhTk-VFUAt6","executionInfo":{"status":"ok","timestamp":1756814694050,"user_tz":-330,"elapsed":2992,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"3daf8a65-e786-4dff-c770-8184f4f3850e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without scaling: 0.7407\n","Accuracy with scaling: 0.9630\n"]}]},{"cell_type":"code","source":["#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n","\n","from sklearn.decomposition import PCA\n","# Scale data before PCA\n","X_scaled = StandardScaler().fit_transform(X)\n","# Apply PCA\n","pca = PCA()\n","pca.fit(X_scaled)\n","# Explained variance ratio\n","print(\"Explained variance ratio of each component:\")\n","print(pca.explained_variance_ratio_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldIcur1dUKKl","executionInfo":{"status":"ok","timestamp":1756814787553,"user_tz":-330,"elapsed":66,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"b35b78e7-123f-4985-ed38-cfb9de38c516"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Explained variance ratio of each component:\n","[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n"," 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n"," 0.00795215]\n"]}]},{"cell_type":"code","source":["#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n","\n","# Retain top 2 components\n","pca_2 = PCA(n_components=2)\n","X_pca = pca_2.fit_transform(X_scaled)\n","# Split PCA data\n","X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n","# Train KNN on PCA data\n","knn.fit(X_train_pca, y_train)\n","y_pred_pca = knn.predict(X_test_pca)\n","acc_pca = accuracy_score(y_test, y_pred_pca)\n","print(f\"Accuracy with top 2 PCA components: {acc_pca:.4f}\")\n","\n","'''\n","Explanation:\n","Reducing to 2 components reduces dimensionality but loses some information, causing a slight drop in accuracy.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BunbQ1AUjrU","executionInfo":{"status":"ok","timestamp":1756814950176,"user_tz":-330,"elapsed":315,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"50c46a97-104f-4aef-fbc8-023302469b55"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with top 2 PCA components: 0.9815\n"]}]},{"cell_type":"code","source":["#Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n","\n","# KNN with Euclidean distance\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train_scaled, y_train)\n","y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n","acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n","# KNN with Manhattan distance\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train_scaled, y_train)\n","y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n","acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n","print(f\"Accuracy with Euclidean distance: {acc_euclidean:.4f}\")\n","print(f\"Accuracy with Manhattan distance: {acc_manhattan:.4f}\")\n","\n","'''\n","Explanation:\n","Both distance metrics perform well, with Euclidean slightly better on this dataset.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmMsNqFTVKDZ","executionInfo":{"status":"ok","timestamp":1756815023496,"user_tz":-330,"elapsed":48,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"96e87db3-11e2-454f-ce9b-399542017773"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Euclidean distance: 0.9630\n","Accuracy with Manhattan distance: 0.9630\n"]}]},{"cell_type":"markdown","source":["---\n","\n","# Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit. Explain how you would:\n","\n","- Use PCA to reduce dimensionality  \n","- Decide how many components to keep  \n","- Use KNN for classification post-dimensionality reduction  \n","- Evaluate the model  \n","- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n","\n","**Answer:**\n","\n","- **Use PCA:**  \n","  Apply PCA to reduce thousands of gene expression features to a smaller set of principal components that capture most variance, reducing noise and redundancy.\n","\n","- **Decide number of components:**  \n","  Select components explaining 90-95% cumulative variance using explained variance ratio or scree plot.\n","\n","- **Use KNN:**  \n","  Train KNN on PCA-transformed data. Reduced dimensions improve distance metric reliability and reduce overfitting.\n","\n","- **Evaluate model:**  \n","  Use cross-validation and metrics like accuracy, precision, recall, F1-score, ROC-AUC. Validate on independent datasets if possible.\n","\n","- **Justification:**  \n","  - PCA mitigates curse of dimensionality and noise.  \n","  - KNN is simple, interpretable, and effective in reduced space.  \n","  - Pipeline is computationally efficient and robust for small sample sizes.  \n","  - Widely accepted in biomedical research for reproducibility and transparency.\n","\n","**Example code snippet:**\n","\n","```python\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Assume X_gene, y_gene are gene expression data and labels\n","X_scaled = StandardScaler().fit_transform(X_gene)\n","pca = PCA(n_components=0.95)  # Retain 95% variance\n","X_pca = pca.fit_transform(X_scaled)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y_gene, test_size=0.3, random_state=42)\n","\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","```\n","\n","---\n","\n","same code below"],"metadata":{"id":"FklGMiz-Vkqw"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Assume X_gene, y_gene are gene expression data and labels\n","X_scaled = StandardScaler().fit_transform(X)\n","pca = PCA(n_components=0.95)  # Retain 95% variance\n","X_pca = pca.fit_transform(X_scaled)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n","\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","y_pred = knn.predict(X_test)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeMEql2VW7rM","executionInfo":{"status":"ok","timestamp":1756815444817,"user_tz":-330,"elapsed":176,"user":{"displayName":"Arshad Hussain","userId":"09062089908931119356"}},"outputId":"3198c5e6-6a06-450c-a4c9-ca52bed825ab"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9629629629629629\n"]}]}]}